{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 6.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rinazbelhaj/EIP/blob/master/Assignment%206/Text_Generation_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXWELk_4omKs",
        "colab_type": "code",
        "outputId": "3ce3dc40-25c0-40db-fc1f-d3a6ff9eb7e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive',force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XXnHgQphzUt",
        "colab_type": "text"
      },
      "source": [
        "# Model as per the Blog"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndPikUjeY35X",
        "colab_type": "code",
        "outputId": "ec84a9c5-0040-492c-8091-6b786d5c83cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Load Larger LSTM network and generate text\n",
        "import sys\n",
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "\n",
        "# load ascii text and covert to lowercase\n",
        "filename = \"gdrive/My Drive/Data/EIP/wonderland.txt\"\n",
        "raw_text = open(filename).read()\n",
        "raw_text = raw_text.lower()\n",
        "\n",
        "# create mapping of unique chars to integers, and a reverse mapping\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "# summarize the loaded data\n",
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total Characters: \", n_chars)\n",
        "print(\"Total Vocab: \", n_vocab)\n",
        "\n",
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "\tseq_in = raw_text[i:i + seq_length]\n",
        "\tseq_out = raw_text[i + seq_length]\n",
        "\tdataX.append([char_to_int[char] for char in seq_in])\n",
        "\tdataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print(\"Total Patterns: \", n_patterns)\n",
        "\n",
        "# reshape X to be [samples, time steps, features]\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = np_utils.to_categorical(dataY)\n",
        "\n",
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "# define the checkpoint\n",
        "filepath=\"best-model.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "# model fit\n",
        "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)\n",
        "\n",
        "# load the network weights\n",
        "filename = \"best-model.hdf5\"\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "# pick a random seed\n",
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print(\"Seed:\")\n",
        "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "\n",
        "print(\"Generated Text : \")\n",
        "print(\"\\n\")\n",
        "# generate characters\n",
        "for i in range(1000):\n",
        "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "\tx = x / float(n_vocab)\n",
        "\tprediction = model.predict(x, verbose=0)\n",
        "\tindex = numpy.argmax(prediction)\n",
        "\tresult = int_to_char[index]\n",
        "\tseq_in = [int_to_char[value] for value in pattern]\n",
        "\tsys.stdout.write(result)\n",
        "\tpattern.append(index)\n",
        "\tpattern = pattern[1:len(pattern)]\n",
        "print(\"\\nDone.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  144442\n",
            "Total Vocab:  46\n",
            "Total Patterns:  144342\n",
            "Epoch 1/20\n",
            "144342/144342 [==============================] - 164s 1ms/step - loss: 2.9568\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.95682, saving model to best-model.hdf5\n",
            "Epoch 2/20\n",
            "144342/144342 [==============================] - 160s 1ms/step - loss: 2.7536\n",
            "\n",
            "Epoch 00002: loss improved from 2.95682 to 2.75359, saving model to best-model.hdf5\n",
            "Epoch 3/20\n",
            "144342/144342 [==============================] - 159s 1ms/step - loss: 2.6502\n",
            "\n",
            "Epoch 00003: loss improved from 2.75359 to 2.65016, saving model to best-model.hdf5\n",
            "Epoch 4/20\n",
            "144342/144342 [==============================] - 158s 1ms/step - loss: 2.5769\n",
            "\n",
            "Epoch 00004: loss improved from 2.65016 to 2.57686, saving model to best-model.hdf5\n",
            "Epoch 5/20\n",
            "144342/144342 [==============================] - 160s 1ms/step - loss: 2.5163\n",
            "\n",
            "Epoch 00005: loss improved from 2.57686 to 2.51626, saving model to best-model.hdf5\n",
            "Epoch 6/20\n",
            "144342/144342 [==============================] - 159s 1ms/step - loss: 2.4612\n",
            "\n",
            "Epoch 00006: loss improved from 2.51626 to 2.46120, saving model to best-model.hdf5\n",
            "Epoch 7/20\n",
            "144342/144342 [==============================] - 157s 1ms/step - loss: 2.4095\n",
            "\n",
            "Epoch 00007: loss improved from 2.46120 to 2.40950, saving model to best-model.hdf5\n",
            "Epoch 8/20\n",
            "144342/144342 [==============================] - 157s 1ms/step - loss: 2.3605\n",
            "\n",
            "Epoch 00008: loss improved from 2.40950 to 2.36051, saving model to best-model.hdf5\n",
            "Epoch 9/20\n",
            "144342/144342 [==============================] - 156s 1ms/step - loss: 2.3156\n",
            "\n",
            "Epoch 00009: loss improved from 2.36051 to 2.31563, saving model to best-model.hdf5\n",
            "Epoch 10/20\n",
            "144342/144342 [==============================] - 160s 1ms/step - loss: 2.2694\n",
            "\n",
            "Epoch 00010: loss improved from 2.31563 to 2.26936, saving model to best-model.hdf5\n",
            "Epoch 11/20\n",
            "144342/144342 [==============================] - 160s 1ms/step - loss: 2.2300\n",
            "\n",
            "Epoch 00011: loss improved from 2.26936 to 2.23002, saving model to best-model.hdf5\n",
            "Epoch 12/20\n",
            "144342/144342 [==============================] - 157s 1ms/step - loss: 2.1875\n",
            "\n",
            "Epoch 00012: loss improved from 2.23002 to 2.18752, saving model to best-model.hdf5\n",
            "Epoch 13/20\n",
            "144342/144342 [==============================] - 156s 1ms/step - loss: 2.1502\n",
            "\n",
            "Epoch 00013: loss improved from 2.18752 to 2.15015, saving model to best-model.hdf5\n",
            "Epoch 14/20\n",
            "144342/144342 [==============================] - 156s 1ms/step - loss: 2.1112\n",
            "\n",
            "Epoch 00014: loss improved from 2.15015 to 2.11120, saving model to best-model.hdf5\n",
            "Epoch 15/20\n",
            "144342/144342 [==============================] - 156s 1ms/step - loss: 2.0749\n",
            "\n",
            "Epoch 00015: loss improved from 2.11120 to 2.07485, saving model to best-model.hdf5\n",
            "Epoch 16/20\n",
            "144342/144342 [==============================] - 156s 1ms/step - loss: 2.0407\n",
            "\n",
            "Epoch 00016: loss improved from 2.07485 to 2.04075, saving model to best-model.hdf5\n",
            "Epoch 17/20\n",
            "144342/144342 [==============================] - 158s 1ms/step - loss: 2.0081\n",
            "\n",
            "Epoch 00017: loss improved from 2.04075 to 2.00808, saving model to best-model.hdf5\n",
            "Epoch 18/20\n",
            "144342/144342 [==============================] - 160s 1ms/step - loss: 1.9785\n",
            "\n",
            "Epoch 00018: loss improved from 2.00808 to 1.97851, saving model to best-model.hdf5\n",
            "Epoch 19/20\n",
            "144342/144342 [==============================] - 156s 1ms/step - loss: 1.9514\n",
            "\n",
            "Epoch 00019: loss improved from 1.97851 to 1.95137, saving model to best-model.hdf5\n",
            "Epoch 20/20\n",
            "144342/144342 [==============================] - 159s 1ms/step - loss: 1.9238\n",
            "\n",
            "Epoch 00020: loss improved from 1.95137 to 1.92380, saving model to best-model.hdf5\n",
            "Seed:\n",
            "\"  went slowly\n",
            "after it: 'i never was so ordered about in all my life, never!'\n",
            "\n",
            "they had not gone far  \"\n",
            "Generated Text : \n",
            "\n",
            "\n",
            "fin hor and toone ano of the woile an in wint hn har hors the har on the toeee on the sime of the soeer sn the woile an inr vonce, \n",
            "'tha lire oh the soite of the boothe of the siater ' the said to herself thehe she wan totiing to the toper of the sime of the sorer. and the seme thin she was not in a lorte to the whrt hnt hore to her  and the seie then she was not in a lorte to the whit had foon the whater whsh the soeer saaling of the simete an soee and auoeessn, and the sere oh the tored oh the soeer on the sime of the soper. and the seme thin she was not in a lort  so toeke and the wiiee saed to the toier on the tame to her on the thaee of the soeee on the sime, and the semeg hrr toe rire  she west hnw and toone of the sime of the soier on the same an inr haad, and the thre oh the toiee oi the sime of the somer on the sime of the soeer on the tame  and saed to tere that she was not of the toier, and the seme oh the sueen sad ift head to tere the horse oo the woole   and the hort as t\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6f1ICQ5h8Yb",
        "colab_type": "text"
      },
      "source": [
        "# Revised Model with the following changes\n",
        "1. Predict 500 characters only\n",
        "2. Remove all the punctuation from the source text\n",
        "3. Train the model on padded sequences rather than random sequences of characters. \n",
        "4. Train the model for 100 epochs\n",
        "5. Add dropout to the input layer, remove it from the layer before dense layer. Use Dropout value of 0.1 everywhere."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfT5XcqsojJs",
        "colab_type": "code",
        "outputId": "c7dc139c-a7a3-4ecf-8107-7bb1844ab9ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Load Larger LSTM network and generate text\n",
        "import sys\n",
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "\n",
        "# load ascii text and covert to lowercase\n",
        "filename = 'gdrive/My Drive/Data/EIP/wonderland.txt'\n",
        "raw_text = open(filename).read()\n",
        "raw_text = raw_text.lower()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_lbMluzEQk3",
        "colab_type": "code",
        "outputId": "afc81ca2-dd6f-4809-f6d8-a70348bc6049",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        }
      },
      "source": [
        "# removing punctuations, \\n , and unnecessary characters\n",
        "import string \n",
        "punctuation = string.punctuation.replace(\".\", \"\") + '\\n' + '\\ufeff'\n",
        "new_text = raw_text.replace(\"\\n\", \" \").translate(str.maketrans('', '', punctuation))\n",
        "\n",
        "# create mapping of unique chars to integers\n",
        "chars = sorted(list(set(new_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "char_to_int"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{' ': 0,\n",
              " '.': 1,\n",
              " '0': 2,\n",
              " '3': 3,\n",
              " 'a': 4,\n",
              " 'b': 5,\n",
              " 'c': 6,\n",
              " 'd': 7,\n",
              " 'e': 8,\n",
              " 'f': 9,\n",
              " 'g': 10,\n",
              " 'h': 11,\n",
              " 'i': 12,\n",
              " 'j': 13,\n",
              " 'k': 14,\n",
              " 'l': 15,\n",
              " 'm': 16,\n",
              " 'n': 17,\n",
              " 'o': 18,\n",
              " 'p': 19,\n",
              " 'q': 20,\n",
              " 'r': 21,\n",
              " 's': 22,\n",
              " 't': 23,\n",
              " 'u': 24,\n",
              " 'v': 25,\n",
              " 'w': 26,\n",
              " 'x': 27,\n",
              " 'y': 28,\n",
              " 'z': 29}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2z2U_4TtjL5",
        "colab_type": "code",
        "outputId": "671bb3f0-fa57-4617-ca67-fe1b4c582ce6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# summarize the loaded data\n",
        "n_chars = len(new_text)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total Characters: \", n_chars)\n",
        "print(\"Total Vocab: \", n_vocab)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  137111\n",
            "Total Vocab:  30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc4HgCMCx9jQ",
        "colab_type": "code",
        "outputId": "ce5fd9a0-4402-412a-af15-ec11646840ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# splitting text into sentences based on fullstops.\n",
        "new_list = new_text.split(\".\") \n",
        "\n",
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for sentence in new_list:\n",
        "  raw_text = sentence\n",
        "  for i in range(0, len(raw_text) , 1):\n",
        "    seq_in = raw_text[i:i + min(len(raw_text)-i-1,seq_length)]\n",
        "    seq_out = raw_text[i + min(len(raw_text)-i-1,seq_length)]\n",
        "    dataX.append([char_to_int[char] for char in seq_in])\n",
        "    dataY.append(char_to_int[seq_out])\n",
        "  n_patterns = len(dataX)\n",
        "dataX = pad_sequences(dataX)\n",
        "print(\"Total Patterns: \", n_patterns)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Patterns:  136121\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuCBTcVIuaB6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reshape X to be [samples, time steps, features]\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = np_utils.to_categorical(dataY)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biIuafYsurQn",
        "colab_type": "code",
        "outputId": "a9cd262f-156a-4a49-a24b-1acbcb781a07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        }
      },
      "source": [
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(Dropout(0.1,input_shape=(X.shape[1], X.shape[2])))\n",
        "model.add(LSTM(256, return_sequences=True))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0727 15:25:20.041573 140073765947264 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0727 15:25:20.068116 140073765947264 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dropout_3 (Dropout)          (None, 100, 1)            0         \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 100, 256)          264192    \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 100, 256)          0         \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 30)                7710      \n",
            "=================================================================\n",
            "Total params: 797,214\n",
            "Trainable params: 797,214\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZAYg3mkuvS1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the checkpoint\n",
        "filepath=\"best-model.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ta-cyGGu0oi",
        "colab_type": "code",
        "outputId": "85977620-6eb9-4380-d434-bec28bf8d2d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# fit the model\n",
        "model.fit(X, y, epochs=100, batch_size=128, callbacks=callbacks_list)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "136121/136121 [==============================] - 278s 2ms/step - loss: 2.8063\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.80634, saving model to best-model.hdf5\n",
            "Epoch 2/100\n",
            "136121/136121 [==============================] - 283s 2ms/step - loss: 2.4587\n",
            "\n",
            "Epoch 00002: loss improved from 2.80634 to 2.45867, saving model to best-model.hdf5\n",
            "Epoch 3/100\n",
            "136121/136121 [==============================] - 283s 2ms/step - loss: 1.8925\n",
            "\n",
            "Epoch 00003: loss improved from 2.45867 to 1.89249, saving model to best-model.hdf5\n",
            "Epoch 4/100\n",
            "136121/136121 [==============================] - 283s 2ms/step - loss: 1.5902\n",
            "\n",
            "Epoch 00004: loss improved from 1.89249 to 1.59018, saving model to best-model.hdf5\n",
            "Epoch 5/100\n",
            "136121/136121 [==============================] - 283s 2ms/step - loss: 1.4516\n",
            "\n",
            "Epoch 00005: loss improved from 1.59018 to 1.45159, saving model to best-model.hdf5\n",
            "Epoch 6/100\n",
            "136121/136121 [==============================] - 283s 2ms/step - loss: 1.3652\n",
            "\n",
            "Epoch 00006: loss improved from 1.45159 to 1.36522, saving model to best-model.hdf5\n",
            "Epoch 7/100\n",
            "136121/136121 [==============================] - 283s 2ms/step - loss: 1.3073\n",
            "\n",
            "Epoch 00007: loss improved from 1.36522 to 1.30730, saving model to best-model.hdf5\n",
            "Epoch 8/100\n",
            "136121/136121 [==============================] - 283s 2ms/step - loss: 1.2609\n",
            "\n",
            "Epoch 00008: loss improved from 1.30730 to 1.26091, saving model to best-model.hdf5\n",
            "Epoch 9/100\n",
            "136121/136121 [==============================] - 283s 2ms/step - loss: 1.2273\n",
            "\n",
            "Epoch 00009: loss improved from 1.26091 to 1.22729, saving model to best-model.hdf5\n",
            "Epoch 10/100\n",
            "136121/136121 [==============================] - 283s 2ms/step - loss: 1.2057\n",
            "\n",
            "Epoch 00010: loss improved from 1.22729 to 1.20572, saving model to best-model.hdf5\n",
            "Epoch 11/100\n",
            "136121/136121 [==============================] - 282s 2ms/step - loss: 1.1694\n",
            "\n",
            "Epoch 00011: loss improved from 1.20572 to 1.16944, saving model to best-model.hdf5\n",
            "Epoch 12/100\n",
            "136121/136121 [==============================] - 282s 2ms/step - loss: 1.1466\n",
            "\n",
            "Epoch 00012: loss improved from 1.16944 to 1.14661, saving model to best-model.hdf5\n",
            "Epoch 13/100\n",
            "136121/136121 [==============================] - 282s 2ms/step - loss: 1.1363\n",
            "\n",
            "Epoch 00013: loss improved from 1.14661 to 1.13626, saving model to best-model.hdf5\n",
            "Epoch 14/100\n",
            "136121/136121 [==============================] - 282s 2ms/step - loss: 1.1149\n",
            "\n",
            "Epoch 00014: loss improved from 1.13626 to 1.11489, saving model to best-model.hdf5\n",
            "Epoch 15/100\n",
            "136121/136121 [==============================] - 282s 2ms/step - loss: 1.1021\n",
            "\n",
            "Epoch 00015: loss improved from 1.11489 to 1.10208, saving model to best-model.hdf5\n",
            "Epoch 16/100\n",
            "136121/136121 [==============================] - 283s 2ms/step - loss: 1.0845\n",
            "\n",
            "Epoch 00016: loss improved from 1.10208 to 1.08451, saving model to best-model.hdf5\n",
            "Epoch 17/100\n",
            "136121/136121 [==============================] - 282s 2ms/step - loss: 1.0681\n",
            "\n",
            "Epoch 00017: loss improved from 1.08451 to 1.06813, saving model to best-model.hdf5\n",
            "Epoch 18/100\n",
            "136121/136121 [==============================] - 282s 2ms/step - loss: 1.0590\n",
            "\n",
            "Epoch 00018: loss improved from 1.06813 to 1.05901, saving model to best-model.hdf5\n",
            "Epoch 19/100\n",
            "136121/136121 [==============================] - 282s 2ms/step - loss: 1.5661\n",
            "\n",
            "Epoch 00019: loss did not improve from 1.05901\n",
            "Epoch 20/100\n",
            "136121/136121 [==============================] - 282s 2ms/step - loss: 2.6825\n",
            "\n",
            "Epoch 00020: loss did not improve from 1.05901\n",
            "Epoch 21/100\n",
            "136121/136121 [==============================] - 282s 2ms/step - loss: 2.6774\n",
            "\n",
            "Epoch 00021: loss did not improve from 1.05901\n",
            "Epoch 22/100\n",
            "136121/136121 [==============================] - 282s 2ms/step - loss: 2.4275\n",
            "\n",
            "Epoch 00022: loss did not improve from 1.05901\n",
            "Epoch 23/100\n",
            "136121/136121 [==============================] - 282s 2ms/step - loss: 1.4197\n",
            "\n",
            "Epoch 00023: loss did not improve from 1.05901\n",
            "Epoch 24/100\n",
            "136121/136121 [==============================] - 282s 2ms/step - loss: 1.3822\n",
            "\n",
            "Epoch 00024: loss did not improve from 1.05901\n",
            "Epoch 25/100\n",
            "136121/136121 [==============================] - 283s 2ms/step - loss: 2.0930\n",
            "\n",
            "Epoch 00025: loss did not improve from 1.05901\n",
            "Epoch 26/100\n",
            "136121/136121 [==============================] - 283s 2ms/step - loss: 2.6655\n",
            "\n",
            "Epoch 00026: loss did not improve from 1.05901\n",
            "Epoch 27/100\n",
            "136121/136121 [==============================] - 287s 2ms/step - loss: 2.6559\n",
            "\n",
            "Epoch 00027: loss did not improve from 1.05901\n",
            "Epoch 28/100\n",
            "136121/136121 [==============================] - 288s 2ms/step - loss: 2.6514\n",
            "\n",
            "Epoch 00028: loss did not improve from 1.05901\n",
            "Epoch 29/100\n",
            "136121/136121 [==============================] - 289s 2ms/step - loss: 2.6481\n",
            "\n",
            "Epoch 00029: loss did not improve from 1.05901\n",
            "Epoch 30/100\n",
            "136121/136121 [==============================] - 288s 2ms/step - loss: 2.6418\n",
            "\n",
            "Epoch 00030: loss did not improve from 1.05901\n",
            "Epoch 31/100\n",
            "136121/136121 [==============================] - 288s 2ms/step - loss: 2.6377\n",
            "\n",
            "Epoch 00031: loss did not improve from 1.05901\n",
            "Epoch 32/100\n",
            "  8576/136121 [>.............................] - ETA: 4:28 - loss: 2.6361"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-35a03041d7ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMIYh25Lu3L4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the network weights\n",
        "filename = \"best-model.hdf5\"\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLL5QhEklPIU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "79845855-220d-463d-88be-82dde810ab97"
      },
      "source": [
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "# pick a random seed\n",
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "pattern = list(dataX[start])\n",
        "print(\"Seed:\")\n",
        "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "print(\"\\n\")\n",
        "print(\"\\nGenerated Text : \")\n",
        "# generate characters\n",
        "for i in range(500):\n",
        "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "\tx = x / float(n_vocab)\n",
        "\tprediction = model.predict(x, verbose=0)\n",
        "\tindex = numpy.argmax(prediction)\n",
        "\tresult = int_to_char[index]\n",
        "\tseq_in = [int_to_char[value] for value in pattern]\n",
        "\tsys.stdout.write(result)\n",
        "\tpattern.append(index)\n",
        "\tpattern = pattern[1:len(pattern)]\n",
        "print(\"\\nDone.\")\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "\"  strange adventures of hers that you have just been reading about and when she had finished her sist \"\n",
            "\n",
            "\n",
            "\n",
            "Generated Text : \n",
            "en the wai  and thatie  and thatie  and thatie  and thatie  and thatie  and thatie  and thatie  and thatie  and thatie  and thatie  and thatie  and thatie  and thatie  and thatie  and thatie  and thatie  and thatie  and thatie  and thatie  and thatie  and thatie  and thatie  and thatie  and thatie  and thatie  and thatie  and thatie  and thatie  and thatie  and thatie  and thatie  and thatie  and thatie  and thatie  and thatie  and thatie  and thatie  and thatie  and thatie  and thatie  and that\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJ7aXuZ-lf4a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}